{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Le preprocessing a été externalisé dans un module Python dédié (src/preprocessing.py) afin de garantir la réutilisabilité du code, la lisibilité du projet et la prévention du data leakage. Ce module est importé dans la phase de modélisation."
      ],
      "metadata": {
        "id": "hyZN10qazg0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZL6qIYevZ0S"
      },
      "outputs": [],
      "source": [
        "%%writefile src/preprocessing.py\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "def load_data(url):\n",
        "    \"\"\"Charge les données depuis GitHub\"\"\"\n",
        "    return pd.read_csv(url, sep=\"\\t\")\n",
        "\n",
        "\n",
        "def build_preprocessor(num_features, cat_features):\n",
        "    \"\"\"Construit le pipeline de preprocessing\"\"\"\n",
        "\n",
        "    numeric_pipeline = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_pipeline = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder\", OneHotEncoder(\n",
        "            handle_unknown=\"ignore\",\n",
        "            sparse_output=False\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_pipeline, num_features),\n",
        "            (\"cat\", categorical_pipeline, cat_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def prepare_train_test(\n",
        "    df,\n",
        "    target_col,\n",
        "    num_features,\n",
        "    cat_features,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        "):\n",
        "    \"\"\"Split + preprocessing sans data leakage\"\"\"\n",
        "\n",
        "    X = df[num_features + cat_features]\n",
        "    y = df[target_col]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=test_size,\n",
        "        stratify=y,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    preprocessor = build_preprocessor(num_features, cat_features)\n",
        "\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    return X_train_processed, X_test_processed, y_train, y_test, preprocessor\n"
      ],
      "metadata": {
        "id": "Wr66Ok7jytrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}